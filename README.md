# BIG-DATA-ANALYSIS

*COMPANY* : CODTECH IT SOLUTIONS

*NAME* : ADITYA KAMTHE

*INTERN ID* : CT06DF2353

*DOMAIN* : DATA ANALYTICS

*DURATION* : 6 WEEKS

*MENTOR* : NEELA SANTOSH

## Big Data Analysis using PySpark - Instagram Dataset (Walmart)

# Overview
This project, "Big Data Analysis - Task 1", demonstrates the use of Apache Spark (PySpark) on Google Colab to analyze Instagram marketing data related to Walmart. The analysis provides insights into post engagement, user behavior, and content effectiveness. The data was sourced from Kaggle, and processed using PySpark's powerful distributed computing capabilities.

The main focus is on handling large-scale data efficiently and deriving key metrics that can help in data-driven decision-making for social media strategies. This project is ideal for understanding practical big data analytics workflows using PySpark in a cloud-based environment.

# Dataset
Source: Kaggle â€“ Walmart Instagram Data

File Used: Instagram data.csv

Fields Included: Caption, Likes, Impressions, Shares, Follows, Hashtags, Profile Visits, etc.

The dataset contains Instagram post data from Walmart, focusing on how different posts perform in terms of reach, engagement, and user response.

# Technologies Used
Google Colab: Cloud-based Python notebook environment.

Apache Spark (PySpark): Big data processing framework.

Spark SQL & MLlib: Used for data manipulation and statistical correlation.

Kaggle API / Manual Upload: For data retrieval.

# Project Workflow
1. Environment Setup
The Colab notebook starts by installing PySpark using pip. A SparkSession is then initialized for session-based operations.

2. Data Loading and Inspection
The Instagram data is loaded from the specified Google Drive path using:

# python
Copy
Edit
df = spark.read.csv(path, header=True, inferSchema=True)
Schema and sample records are displayed using .printSchema() and .show().

3. Data Cleaning and Summary
Missing Values: Counted using isNull() and when() functions.

Summary Stats: Computed using .describe() for numerical columns.

4. Data Analysis
Top Performing Posts: Sorted by likes and impressions.

Correlation Analysis: Correlation between likes and follows using VectorAssembler and Correlation.corr.

Group Averages: Mean of metrics like impressions, likes, follows, etc.

5. Hashtag Analysis
Hashtags are extracted, cleaned (lowercased and trimmed), and split using PySpark string functions.

Most frequent hashtags are identified and sorted by count.

6. Result Export
Key insights (like captions with impressions and likes) are exported as a CSV file for further use or reporting.

Output
CSV file containing summarized insights (insights_output/)

Printed metrics like correlation matrix, null value summary, and top hashtags

Visual insights can be generated by importing this CSV into tools like Power BI, Tableau, or Excel.

# Conclusion

This project demonstrates the practical application of PySpark for analyzing large-scale social media datasets on Google Colab. It provides a scalable and efficient approach to processing, cleaning, and analyzing real-world data to generate actionable business insights for brands like Walmart. The code structure also makes it easy to extend for more advanced machine learning or visualization tasks.
